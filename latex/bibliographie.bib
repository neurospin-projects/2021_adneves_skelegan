
@article{noauthor_notitle_nodate,
}

@article{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2021-08-17},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/M2VTKM79/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/KIMG5I2F/1406.html:text/html},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journaltitle = {{arXiv}:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2021-08-17},
	date = {2016-01-07},
	eprinttype = {arxiv},
	eprint = {1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/5HYDDLPH/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/AHEHAPET/1511.html:text/html},
}

@article{mirza_conditional_2014,
	title = {Conditional Generative Adversarial Nets},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate {MNIST} digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	journaltitle = {{arXiv}:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	urldate = {2021-08-17},
	date = {2014-11-06},
	eprinttype = {arxiv},
	eprint = {1411.1784},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/EQDG953E/Mirza et Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/EB3E76HK/1411.html:text/html},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named {WGAN}, an alternative to traditional {GAN} training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	journaltitle = {{arXiv}:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	urldate = {2021-08-17},
	date = {2017-12-06},
	eprinttype = {arxiv},
	eprint = {1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/BCJWY5WX/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/W338DQC9/1701.html:text/html},
}

@article{isola_image--image_2018,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2021-08-17},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/UEACVXSZ/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/KK9EF7SI/1611.html:text/html},
}

@article{liu_unsupervised_2018,
	title = {Unsupervised Image-to-Image Translation Networks},
	url = {http://arxiv.org/abs/1703.00848},
	abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled {GANs}. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit .},
	journaltitle = {{arXiv}:1703.00848 [cs]},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	urldate = {2021-08-17},
	date = {2018-07-22},
	eprinttype = {arxiv},
	eprint = {1703.00848},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/79R57CSG/Liu et al. - 2018 - Unsupervised Image-to-Image Translation Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/8SKC4TG8/1703.html:text/html},
}

@article{choi_stargan_2018,
	title = {{StarGAN}: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
	url = {http://arxiv.org/abs/1711.09020},
	shorttitle = {{StarGAN}},
	abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose {StarGAN}, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of {StarGAN} allows simultaneous training of multiple datasets with different domains within a single network. This leads to {StarGAN}'s superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
	journaltitle = {{arXiv}:1711.09020 [cs]},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	urldate = {2021-08-17},
	date = {2018-09-21},
	eprinttype = {arxiv},
	eprint = {1711.09020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/7WZL2X2P/Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks f.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/T52VXVWZ/1711.html:text/html},
}

@article{choi_stargan_2020,
	title = {{StarGAN} v2: Diverse Image Synthesis for Multiple Domains},
	url = {http://arxiv.org/abs/1912.01865},
	shorttitle = {{StarGAN} v2},
	abstract = {A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose {StarGAN} v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on {CelebA}-{HQ} and a new animal faces dataset ({AFHQ}) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release {AFHQ}, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.},
	journaltitle = {{arXiv}:1912.01865 [cs]},
	author = {Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
	urldate = {2021-08-17},
	date = {2020-04-26},
	eprinttype = {arxiv},
	eprint = {1912.01865},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/4RVKQ4IV/Choi et al. - 2020 - StarGAN v2 Diverse Image Synthesis for Multiple D.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/XSXZSV48/1912.html:text/html},
}

@article{milletari_v-net_2016,
	title = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},
	url = {http://arxiv.org/abs/1606.04797},
	shorttitle = {V-Net},
	abstract = {Convolutional Neural Networks ({CNNs}) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our {CNN} is trained end-to-end on {MRI} volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
	journaltitle = {{arXiv}:1606.04797 [cs]},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	urldate = {2021-08-17},
	date = {2016-06-15},
	eprinttype = {arxiv},
	eprint = {1606.04797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/6VSAG4EU/Milletari et al. - 2016 - V-Net Fully Convolutional Neural Networks for Vol.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/BD9M2Y9N/1606.html:text/html},
}

@article{schlegl_unsupervised_2017,
	title = {Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery},
	url = {http://arxiv.org/abs/1703.05921},
	abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose {AnoGAN}, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
	journaltitle = {{arXiv}:1703.05921 [cs]},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
	urldate = {2021-08-17},
	date = {2017-03-17},
	eprinttype = {arxiv},
	eprint = {1703.05921},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/W26JPTWD/Schlegl et al. - 2017 - Unsupervised Anomaly Detection with Generative Adv.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/5K64HS59/1703.html:text/html},
}

@article{wang_wgan-based_2019,
	title = {{WGAN}-Based Synthetic Minority Over-Sampling Technique: Improving Semantic Fine-Grained Classification for Lung Nodules in {CT} Images},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8629907/},
	doi = {10.1109/ACCESS.2019.2896409},
	shorttitle = {{WGAN}-Based Synthetic Minority Over-Sampling Technique},
	pages = {18450--18463},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Wang, Qingfeng and Zhou, Xuehai and Wang, Chao and Liu, Zhiqin and Huang, Jun and Zhou, Ying and Li, Changlong and Zhuang, Hang and Cheng, Jie-Zhi},
	urldate = {2021-08-17},
	date = {2019},
	file = {Texte intégral:/home/ad265693/Zotero/storage/G83LQPG9/Wang et al. - 2019 - WGAN-Based Synthetic Minority Over-Sampling Techni.pdf:application/pdf},
}

@article{hong_3d-stylegan_2021,
	title = {3D-{StyleGAN}: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images},
	url = {http://arxiv.org/abs/2107.09700},
	shorttitle = {3D-{StyleGAN}},
	abstract = {Image synthesis via Generative Adversarial Networks ({GANs}) of three-dimensional (3D) medical images has great potential that can be extended to many medical applications, such as, image enhancement and disease progression modeling. However, current {GAN} technologies for 3D medical image synthesis need to be significantly improved to be readily adapted to real-world medical problems. In this paper, we extend the state-of-the-art {StyleGAN}2 model, which natively works with two-dimensional images, to enable 3D image synthesis. In addition to the image synthesis, we investigate the controllability and interpretability of the 3D-{StyleGAN} via style vectors inherited form the original {StyleGAN}2 that are highly suitable for medical applications: (i) the latent space projection and reconstruction of unseen real images, and (ii) style mixing. We demonstrate the 3D-{StyleGAN}'s performance and feasibility with {\textasciitilde}12,000 three-dimensional full brain {MR} T1 images, although it can be applied to any 3D volumetric images. Furthermore, we explore different configurations of hyperparameters to investigate potential improvement of the image synthesis with larger networks. The codes and pre-trained networks are available online: https://github.com/sh4174/3DStyleGAN.},
	journaltitle = {{arXiv}:2107.09700 [cs, eess]},
	author = {Hong, Sungmin and Marinescu, Razvan and Dalca, Adrian V. and Bonkhoff, Anna K. and Bretzner, Martin and Rost, Natalia S. and Golland, Polina},
	urldate = {2021-08-17},
	date = {2021-07-20},
	eprinttype = {arxiv},
	eprint = {2107.09700},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, 68T07 (Primary) 68T01 (Secondary), I.2, I.4},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/Y2TGCHSK/Hong et al. - 2021 - 3D-StyleGAN A Style-Based Generative Adversarial .pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/53KKVGNA/2107.html:text/html},
}

@article{liu_coupled_2016,
	title = {Coupled Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1606.07536},
	abstract = {We propose coupled generative adversarial network ({CoGAN}) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, {CoGAN} can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply {CoGAN} to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.},
	journaltitle = {{arXiv}:1606.07536 [cs]},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	urldate = {2021-08-17},
	date = {2016-09-20},
	eprinttype = {arxiv},
	eprint = {1606.07536},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/8MC7D3CG/Liu et Tuzel - 2016 - Coupled Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/VIASKURC/1606.html:text/html},
}

@article{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	shorttitle = {U-Net},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	journaltitle = {{arXiv}:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2021-08-17},
	date = {2015-05-18},
	eprinttype = {arxiv},
	eprint = {1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/S96EC9M5/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/GFQ5VD8Y/1505.html:text/html},
}

@article{dorner_wgan-based_2020,
	title = {{WGAN}-based Autoencoder Training Over-the-air},
	url = {http://arxiv.org/abs/2003.02744},
	abstract = {The practical realization of end-to-end training of communication systems is fundamentally limited by its accessibility of the channel gradient. To overcome this major burden, the idea of generative adversarial networks ({GANs}) that learn to mimic the actual channel behavior has been recently proposed in the literature. Contrarily to handcrafted classical channel modeling, which can never fully capture the real world, {GANs} promise, in principle, the ability to learn any physical impairment, enabled by the data-driven learning algorithm. In this work, we verify the concept of {GAN}-based autoencoder training in actual over-the-air ({OTA}) measurements. To improve training stability, we first extend the concept to conditional Wasserstein {GANs} and embed it into a state-of-the-art autoencoder-architecture, including bitwise estimates and an outer channel code. Further, in the same framework, we compare the existing three different training approaches: model-based pre-training with receiver finetuning, reinforcement learning ({RL}) and {GAN}-based channel modeling. For this, we show advantages and limitations of {GAN}-based end-to-end training. In particular, for non-linear effects, it turns out that learning the whole exploration space becomes prohibitively complex. Finally, we show that the training strategy benefits from a simpler (training) data acquisition when compared to {RL}-based training, which requires continuous transmitter weight updates. This becomes an important practical bottleneck due to limited bandwidth and latency between transmitter and training algorithm that may even operate at physically different locations.},
	journaltitle = {{arXiv}:2003.02744 [cs, eess, math]},
	author = {Dörner, Sebastian and Henninger, Marcus and Cammerer, Sebastian and Brink, Stephan ten},
	urldate = {2021-08-17},
	date = {2020-03-05},
	eprinttype = {arxiv},
	eprint = {2003.02744},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/JCU4FDR4/Dörner et al. - 2020 - WGAN-based Autoencoder Training Over-the-air.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/6TLHWFKW/2003.html:text/html},
}

@article{gulrajani_improved_2017,
	title = {Improved Training of Wasserstein {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks ({GANs}) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein {GAN} ({WGAN}) makes progress toward stable training of {GANs}, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in {WGAN} to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard {WGAN} and enables stable training of a wide variety of {GAN} architectures with almost no hyperparameter tuning, including 101-layer {ResNets} and language models over discrete data. We also achieve high quality generations on {CIFAR}-10 and {LSUN} bedrooms.},
	journaltitle = {{arXiv}:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	urldate = {2021-08-17},
	date = {2017-12-25},
	eprinttype = {arxiv},
	eprint = {1704.00028},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ad265693/Zotero/storage/QMMUHUC5/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:/home/ad265693/Zotero/storage/82K43S84/1704.html:text/html},
}

@article{schlegl_f-anogan_2019,
	title = {f-{AnoGAN}: Fast unsupervised anomaly detection with generative adversarial networks},
	volume = {54},
	issn = {1361-8423},
	doi = {10.1016/j.media.2019.01.010},
	shorttitle = {f-{AnoGAN}},
	abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast {AnoGAN} (f-{AnoGAN}), a generative adversarial network ({GAN}) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the {GAN}'s latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model - comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-{AnoGAN} outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal {OCT} images. The f-{AnoGAN} code is available at https://github.com/{tSchlegl}/f-{AnoGAN}.},
	pages = {30--44},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Med Image Anal},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Langs, Georg and Schmidt-Erfurth, Ursula},
	date = {2019-05},
	pmid = {30831356},
	keywords = {Unsupervised learning, Algorithms, Anomaly detection, Diagnostic Techniques, Ophthalmological, Humans, Image Processing, Computer-Assisted, Information Theory, Neural Networks, Computer, Optical coherence tomography, Retina, Tomography, Optical Coherence, Wasserstein generative adversarial network},
}

@article{bertoux_sulcal_2019,
	title = {Sulcal morphology in Alzheimer's disease: an effective marker of diagnosis and cognition},
	volume = {84},
	issn = {01974580},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0197458019302192},
	doi = {10.1016/j.neurobiolaging.2019.07.015},
	shorttitle = {Sulcal morphology in Alzheimer's disease},
	pages = {41--49},
	journaltitle = {Neurobiology of Aging},
	shortjournal = {Neurobiology of Aging},
	author = {Bertoux, Maxime and Lagarde, Julien and Corlier, Fabian and Hamelin, Lorraine and Mangin, Jean-François and Colliot, Olivier and Chupin, Marie and Braskie, Meredith N. and Thompson, Paul M. and Bottlaender, Michel and Sarazin, Marie},
	urldate = {2021-08-17},
	date = {2019-12},
	langid = {english},
	file = {Version soumise:/home/ad265693/Zotero/storage/N6S8MZRL/Bertoux et al. - 2019 - Sulcal morphology in Alzheimer's disease an effec.pdf:application/pdf},
}